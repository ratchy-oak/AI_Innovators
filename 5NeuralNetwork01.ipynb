{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #4CAF50; color: white; padding: 10px; border-radius: 5px; display: flex; align-items: center;\">\n",
    "    <h1 style=\"margin: 0 auto; font-size: 30px; font-weight: bold; font-family: Helvetica\">การวิเคราะห์ข้อมูลด้วยโครงข่ายประสาทเทียม (Neural Network)</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 25px; font-weight: bold; font-family: Helvetica\">โครงข่ายประสาทเทียม (Neural Network) คืออะไร</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks (โครงข่ายประสาทเทียม) คือโมเดลการคำนวณที่ได้รับแรงบันดาลใจจากการทำงานของสมองมนุษย์ \n",
    "\n",
    "โครงข่ายประสาทเทียมประกอบด้วยหน่วยคำนวณเล็กๆ ที่เรียกว่า นิวรอน (Neuron) ซึ่งคล้ายกับเซลล์ประสาทในสมอง นิวรอนเหล่านี้ถูกจัดเรียงเป็นชั้นๆ โดยทั่วไปจะแบ่งออกเป็น 3 ส่วนหลัก ได้แก่\n",
    "\n",
    "- Input Layer: เป็นชั้นที่รับข้อมูลเข้าไปในระบบ\n",
    "\n",
    "- Hidden Layer(s): เป็นชั้นที่อยู่ระหว่างชั้นข้อมูลเข้าและชั้นผลลัพธ์ เป็นส่วนที่คำนวณและประมวลผลข้อมูล ซึ่งอาจมีหลายชั้นเรียกว่าระบบ Deep Learning ถ้ามีหลายชั้น\n",
    "\n",
    "- Output Layer: เป็นชั้นที่ให้ผลลัพธ์ออกมาจากระบบ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SCz0aTETjTYC864Bqjt6Og.png\" alt=\"Regression\" width=\"1200\" height=\"600\"/>\n",
    "    <br>\n",
    "    <span style=\"font-size: small;\">แผนภาพ Neural Network เบื้องต้น</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 25px; font-weight: bold; font-family: Helvetica\">เครื่องมือที่จะใช้ใน Lab นี้:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# นำเข้าไลบรารี numpy โดยตั้งชื่อว่า np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 25px; font-weight: bold; font-family: Helvetica\">โจทย์ปัญหา:</h1>\n",
    "\n",
    "สมมติว่าเราต้องการโมเดลที่สามารถทำนาย Y โดยมีข้อมูลฟีเจอร์ X ที่กำหนดให้ 3 ข้อมูล ดังนี้:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>X1</th>\n",
    "            <th>X2</th>\n",
    "            <th>X3</th>\n",
    "            <th>Y</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>2.5</td>\n",
    "            <td>1.3</td>\n",
    "            <td>3.8</td>\n",
    "            <td>0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1.7</td>\n",
    "            <td>2.8</td>\n",
    "            <td>4.5</td>\n",
    "            <td>1</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>3.1</td>\n",
    "            <td>1.0</td>\n",
    "            <td>4.1</td>\n",
    "            <td>0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>2.0</td>\n",
    "            <td>3.2</td>\n",
    "            <td>5.2</td>\n",
    "            <td>1</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1.6</td>\n",
    "            <td>2.0</td>\n",
    "            <td>3.6</td>\n",
    "            <td>0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>3.3</td>\n",
    "            <td>1.5</td>\n",
    "            <td>4.8</td>\n",
    "            <td>1</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>2.8</td>\n",
    "            <td>2.4</td>\n",
    "            <td>5.2</td>\n",
    "            <td>0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1.2</td>\n",
    "            <td>2.5</td>\n",
    "            <td>3.7</td>\n",
    "            <td>1</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>3.0</td>\n",
    "            <td>1.9</td>\n",
    "            <td>4.9</td>\n",
    "            <td>0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>2.3</td>\n",
    "            <td>3.0</td>\n",
    "            <td>5.3</td>\n",
    "            <td>1</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 25px; font-weight: bold; font-family: Helvetica\">1. นิวรอน (Neuron)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "นิวรอน (Neuron) ในบริบทของโครงข่ายประสาทเทียม (Neural Networks) คือหน่วยคำนวณพื้นฐานที่ใช้ในการประมวลผลข้อมูลและสร้างผลลัพธ์\n",
    "\n",
    "โดยนิวรอนจะรับข้อมูลจากนิวรอนอื่นๆ (หรือจากข้อมูลนำเข้าในชั้นแรก) คำนวณผลลัพธ์จากข้อมูลเหล่านั้น และส่งต่อผลลัพธ์ไปยังนิวรอนอื่นๆ ในชั้นถัดไป โดยนิวรอนจะมีองค์ประกอบหลักๆดังนี้:\n",
    "\n",
    "- **น้ำหนัก (Weights):** เป็นค่าที่แสดงถึงความสำคัญของข้อมูลที่นิวรอนรับเข้ามา นิวรอนจะปรับค่าน้ำหนักเหล่านี้ระหว่างการฝึก (Training) เพื่อเพิ่มความแม่นยำของโมเดล \n",
    "\n",
    "     สูตรการคำนวณผลรวมของนิวรอน หรือ $z$ :\n",
    "      \n",
    "     <div style=\"font-size: 30px;\">\n",
    "\n",
    "     $$\n",
    "     z = \\sum_{i=1}^{n} (w_i \\times x_i) + b\n",
    "     $$\n",
    "\n",
    "     </div>\n",
    "     \n",
    "     โดยที่:\n",
    "     \n",
    "     - $ w_i $ คือค่าน้ำหนัก\n",
    "\n",
    "     - $ x_i $ คือค่าข้อมูลนำเข้า\n",
    "     \n",
    "     - $ b $ คือค่า bias\n",
    "\n",
    "- **ฟังก์ชันการกระตุ้น (Activation Function):** ทำหน้าที่เปลี่ยนแปลงค่าที่ได้จากการรวมค่า $z$ ให้กลายเป็นค่าที่จะถูกส่งออกไปยังชั้นถัดไป เช่น\n",
    "\n",
    "     - **ReLU**: \n",
    "\n",
    "     <div style=\"font-size: 24px;\">\n",
    "\n",
    "     $$ \\text{ReLU}(z) = \\max(0, z) $$\n",
    "\n",
    "     </div>\n",
    "\n",
    "     - **Sigmoid**: \n",
    "     \n",
    "     <div style=\"font-size: 24px;\">\n",
    "\n",
    "     $$ \\text{Sigmoid}(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "     </div>\n",
    "\n",
    "     - **Softmax**: \n",
    "\n",
    "     <div style=\"font-size: 24px;\">\n",
    "\n",
    "     $$ \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}} $$\n",
    "\n",
    "     </div>\n",
    "     \n",
    "- **เอาต์พุต (Output):** คือผลลัพธ์ที่ได้หลังจากผ่านฟังก์ชันการกระตุ้น (Activation Function) และจะถูกส่งไปยังนิวรอนในชั้นถัดไปหรือใช้เป็นผลลัพธ์สุดท้ายของเครือข่าย"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:640/1*sPg-0hha7o3iNPjY4n-vow.jpeg\" alt=\"Regression\" width=\"700\" height=\"420\"/>\n",
    "    <br>\n",
    "    <span style=\"font-size: small;\">แผนภาพ Neuron    เบื้องต้น</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 25px; font-weight: bold; font-family: Helvetica\">2. การคำนวณไปข้างหน้า (Forward Propagation)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "การคำนวณไปข้างหน้า (Forward Propagation) ซึ่งหมายถึงกระบวนการที่ข้อมูล (inputs) ถูกส่งผ่านไปตามชั้นของโมเดล (เช่น neuron หรือ layer ต่างๆ) เพื่อคำนวณผลลัพธ์ (output)\n",
    "\n",
    "โดยทั่วไปแล้ว การส่งผ่านข้อมูลไปข้างหน้าจะเริ่มจากการนำค่า input คูณกับค่าถ่วงน้ำหนัก (weights) ของ neuron และบวกกับค่า bias \n",
    "\n",
    "จากนั้นจะนำค่าเหล่านี้ไปผ่านฟังก์ชันการกระตุ้น (activation function) เพื่อสร้าง output สำหรับชั้นนั้นๆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ข้อมูล Features จากตาราง\n",
    "X = np.array([\n",
    "    [2.5, 1.3, 3.8],\n",
    "    [1.7, 2.8, 4.5],\n",
    "    [3.1, 1.0, 4.1],\n",
    "    [2.0, 3.2, 5.2],\n",
    "    [1.6, 2.0, 3.6],\n",
    "    [3.3, 1.5, 4.8],\n",
    "    [2.8, 2.4, 5.2],\n",
    "    [1.2, 2.5, 3.7],\n",
    "    [3.0, 1.9, 4.9],\n",
    "    [2.3, 3.0, 5.3]\n",
    "])\n",
    "\n",
    "# ผลลัพธ์ที่แท้จริง (Labels)\n",
    "y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n",
    "\n",
    "# แยกข้อมูลเป็นอาร์เรย์ของ x1, x2, x3\n",
    "x1 = X[:, 0]\n",
    "x2 = X[:, 1]\n",
    "x3 = X[:, 2]\n",
    "\n",
    "# ฟังก์ชันการกระตุ้นแบบ Sigmoid\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ฟังก์ชันการกระตุ้นแบบ ReLU\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "# ฟังก์ชันการกระตุ้นแบบ Softmax\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 18px;\">2.1 การทำ Forward Propagation แบบไม่มี Hidden Layer</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# การทำ Forward Propagation แบบมี Neuron เพียง 1 โหนด\n",
    "\n",
    "# ค่าถ่วงน้ำหนัก (Weights)\n",
    "w1 = 0.5; w2 = -0.6; w3 = 0.2\n",
    "\n",
    "# ค่า bias\n",
    "b = -0.1\n",
    "\n",
    "# การคำนวณ z\n",
    "z = w1 * x1 + w2 * x2 + w3 * x3 + b\n",
    "\n",
    "# การคำนวณผลลัพธ์โดยใช้ฟังก์ชันการกระตุ้น\n",
    "a_relu = relu(z)\n",
    "a_sigmoid = sigmoid(z)\n",
    "\n",
    "print(\"ค่า z:\", z)\n",
    "print()\n",
    "print(\"ค่า a (Output หลังจาก ReLU):\", a_relu)\n",
    "print()\n",
    "print(\"ค่า a (Output หลังจาก Sigmoid):\", a_sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# การทำ Forward Propagation แบบมี Neuron 2 โหนด\n",
    "\n",
    "# ค่าถ่วงน้ำหนัก (Weights) และ bias สำหรับ 2 โหนด\n",
    "weights = np.array([\n",
    "    [0.5, -0.6, 0.2],  # โหนดที่ 1\n",
    "    [-0.3, 0.8, 0.1],  # โหนดที่ 2\n",
    "])\n",
    "\n",
    "biases = np.array([-0.1, 0.2])\n",
    "\n",
    "# การคำนวณ z สำหรับแต่ละโหนด\n",
    "z = np.dot(X, weights.T) + biases\n",
    "\n",
    "# การคำนวณผลลัพธ์โดยใช้ฟังก์ชันการกระตุ้น\n",
    "a_softmax = softmax(z)\n",
    "a_relu = relu(z)\n",
    "\n",
    "print(\"ค่า z สำหรับโหนดทั้ง 3:\", z)\n",
    "print()\n",
    "print(\"ค่า a (Output หลังจาก ReLU):\", a_relu)\n",
    "print()\n",
    "print(\"ค่า a (Output หลังจาก Softmax):\", a_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUIZ:** จงสร้าง Neural Network ที่ Output layer มีทั้งหมด 3 โหนด จากนั้นทำ Forward Propagation กับข้อมูลตารางข้างต้น และทุกโหนดใช้ฟังก์ชันการกระตุ้นเป็น Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#โค้ดคำตอบที่นี่\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 18px;\">2.2 การทำ Forward Propagation แบบมี Hidden Layer</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# การทำ Forward Propagation แบบมี Hidden layer 1 ชั้น ซึ่งมี Neuron 2 โหนด และ Output layer มี Neuron 1 โหนด \n",
    "\n",
    "# ค่าถ่วงน้ำหนัก (Weights) และ bias สำหรับ Hidden Layer (2 โหนด)\n",
    "weights_hidden_1 = np.array([\n",
    "    [0.5, -0.6, 0.2],  # โหนดที่ 1\n",
    "    [-0.3, 0.8, 0.1]   # โหนดที่ 2\n",
    "])\n",
    "\n",
    "biases_hidden_1 = np.array([-0.1, 0.2])\n",
    "\n",
    "# ค่าถ่วงน้ำหนักและ bias สำหรับ Output Layer (1 โหนด)\n",
    "weights_output = np.array([0.4, -0.7])\n",
    "bias_output = -0.3\n",
    "\n",
    "# การคำนวณ z สำหรับ Hidden Layer (2 โหนด)\n",
    "z_hidden_1 = np.dot(X, weights_hidden_1.T) + biases_hidden_1\n",
    "\n",
    "# การคำนวณผลลัพธ์ของ Hidden Layer โดยใช้ฟังก์ชันการกระตุ้น (ReLU)\n",
    "a_hidden_1 = relu(z_hidden_1)\n",
    "\n",
    "# การคำนวณ z สำหรับ Output Layer (1 โหนด)\n",
    "z_output = np.dot(a_hidden_1, weights_output.T) + bias_output\n",
    "\n",
    "# การคำนวณผลลัพธ์ของ Output Layer โดยใช้ฟังก์ชันการกระตุ้น (Sigmoid)\n",
    "a_output = sigmoid(z_output)\n",
    "\n",
    "print(\"ค่า z สำหรับ Hidden Layer (2 โหนด):\", z_hidden_1)\n",
    "print()\n",
    "print(\"ค่า a (Output หลังจาก ReLU สำหรับ Hidden Layer):\", a_hidden_1)\n",
    "print()\n",
    "print(\"ค่า z สำหรับ Output Layer (1 โหนด):\", z_output)\n",
    "print()\n",
    "print(\"ค่า a (Output หลังจาก Sigmoid สำหรับ Output Layer):\", a_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUIZ:** จงสร้าง Neural Network ที่มี hidden layer 1 ชั้น ซึ่งมีทั้งหมด 3 โหนด และ Output layer มีทั้งหมด 2 โหนด จากนั้นทำ Forward Propagation กับข้อมูลตารางข้างต้น\n",
    "\n",
    "โดยทุกโหนดใน Hidden layer จะผ่าน Relu และทุกโหนดใน Output layer จะผ่าน Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#โค้ดคำตอบที่นี่\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 25px; font-weight: bold; font-family: Helvetica\">3. การคำนวณย้อนกลับ (Backward Propagation)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**การคำนวณย้อนกลับ** หรือ **Backward Propagation** เป็นขั้นตอนสำคัญในกระบวนการฝึกอบรม (training) ของโมเดล neural network โดยเฉพาะในการปรับค่าถ่วงน้ำหนัก (weights) และค่า bias ให้กับโมเดล\n",
    "\n",
    "โดยขั้นตอนการทำงานของ Backward Propagation มีดังนี้\n",
    "\n",
    "1. **คำนวณความสูญเสีย (Loss Calculation)**: \n",
    "   - คำนวณค่าความสูญเสียจากการเปรียบเทียบค่าผลลัพธ์ที่โมเดลทำนายกับค่าผลลัพธ์จริง (labels)\n",
    "   \n",
    "2. **คำนวณ Gradient**:\n",
    "   - คำนวณค่า gradient ของความสูญเสียตามค่าถ่วงน้ำหนักและ bias ในแต่ละชั้นของโมเดล\n",
    "  \n",
    "3. **ปรับปรุงค่าถ่วงน้ำหนักและ bias**:\n",
    "   - ใช้ค่า gradient ที่คำนวณได้ในการปรับปรุงค่าถ่วงน้ำหนักและ bias เพื่อให้ความสูญเสียลดลงในการทำนายครั้งถัดไป (โดยใช้ Gradient Descent)\n",
    "\n",
    "4. **การส่งข้อมูลย้อนกลับ**:\n",
    "   - กระบวนการปรับปรุงนี้เริ่มจากชั้นสุดท้ายของโมเดลย้อนกลับไปยังชั้นแรก นี่จึงเป็นที่มาของคำว่า \"Backward Propagation\" หรือ \"การส่งผ่านข้อมูลย้อนกลับ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 18px;\">3.1 คำนวณค่า Cost ที่เกิดขึ้น</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 18px;\">ในที่นี้จะใช้ Binary Crossentropy เนื่องจากโจทย์ของเราเป็นการทำ Binary Classification</h1>\n",
    "<h1 style=\"font-size: 18px;\">Binary Crossentropy สามารถคำนวณได้ดังนี้:</h1>\n",
    "\n",
    "<div style=\"font-size: 24px;\">\n",
    "\n",
    "$$ J(w,b) = - \\frac{1}{m} \\sum_{i=1}^{m} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)] $$\n",
    "\n",
    "</div>\n",
    "\n",
    "โดยที่:\n",
    "\n",
    "- $J(w,b)$ คือค่า Cost โดยในที่นี้เราใช้เป็น Binary Crossentropy Loss\n",
    "- $m$ คือจำนวนข้อมูล\n",
    "- $y_i$ คือค่าจริง (actual) สำหรับข้อมูลลำดับที่ $i$\n",
    "- $\\hat{y}_i$ คือค่าที่ทำนาย (predict) สำหรับข้อมูลลำดับที่ $i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# การคำนวณ Binary Cross-Entropy (BCE)\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    bce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return bce\n",
    "\n",
    "# คำนวณ BCE จาก a_output (ผลลัพธ์ที่ผ่าน Sigmoid)\n",
    "bce = binary_cross_entropy(y, a_output)\n",
    "\n",
    "print(\"ค่า Binary Cross-Entropy (BCE):\", bce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 18px;\">3.2 คำนวณ Gradient สำหรับแต่ละโหนดใน Backward Propagation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.1 คำนวณ Gradient ของโหนดในชั้น Output\n",
    "\n",
    "ให้:\n",
    "- `L` เป็นค่า loss\n",
    "\n",
    "- `a_output` เป็นผลลัพธ์ที่ได้จากฟังก์ชัน Sigmoid ของโหนดสุดท้าย\n",
    "\n",
    "- `y` เป็นค่าผลลัพธ์จริง (label)\n",
    "\n",
    "- `z_output` เป็นค่า z ของโหนดสุดท้าย\n",
    "\n",
    "Gradient ของ Loss ตามค่า `z_output` คำนวณได้จากสมการ:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_{\\text{output}}} = a_{\\text{output}} - y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# คำนวณ Gradient ของ Loss ตามค่า z_output\n",
    "dz_output = a_output - y\n",
    "print(dz_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากนั้นคำนวณ Gradient ของค่าถ่วงน้ำหนัก `weights_output` และ bias `bias_output`:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{\\text{output}}} = \\frac{\\partial L}{\\partial z_{\\text{output}}} \\cdot a_{\\text{hidden\\_1}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_{\\text{output}}} = \\frac{\\partial L}{\\partial z_{\\text{output}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# คำนวณ Gradient ของ w_output และ b_output\n",
    "dw_output = np.dot(dz_output.T, a_hidden_1)\n",
    "db_output = np.sum(dz_output, axis=0)\n",
    "print(dw_output)\n",
    "print(db_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.2 คำนวณ Gradient ของโหนดในชั้น Hidden Layer\n",
    "\n",
    "ให้:\n",
    "- `z_hidden_1` เป็นค่า z ของโหนดในชั้นแรก\n",
    "\n",
    "Gradient ของ Loss ตามค่า `z_hidden_1` คำนวณได้ดังนี้:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_{\\text{hidden\\_1}}} = \\frac{\\partial L}{\\partial z_{\\text{output}}} \\cdot w_{\\text{output}} \\cdot \\text{ReLU'}(z_{\\text{hidden\\_1}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# คำนวณ ReLU' สำหรับ z_hidden_1\n",
    "relu_prime = z_hidden_1 > 0\n",
    "\n",
    "# ตรวจสอบขนาดของ dz_output ให้เป็น (batch_size, 1)\n",
    "dz_output = dz_output.reshape(-1, 1)\n",
    "\n",
    "# ตรวจสอบขนาดของ weights_output ให้เป็น (1, 2) เพื่อให้ตรงกับขนาดในการคูณ\n",
    "weights_output_reshape = weights_output.reshape(1, -1)\n",
    "\n",
    "# คำนวณ Gradient ของ z_hidden_1\n",
    "dz_hidden_1 = np.dot(dz_output, weights_output_reshape) * relu_prime\n",
    "\n",
    "print(dz_hidden_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากนั้นคำนวณ Gradient ของค่าถ่วงน้ำหนัก `weights_hidden_1` และ bias `biases_hidden_1`:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{\\text{hidden\\_1}}} = \\frac{\\partial L}{\\partial z_{\\text{hidden\\_1}}} \\cdot X\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_{\\text{hidden\\_1}}} = \\frac{\\partial L}{\\partial z_{\\text{hidden\\_1}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_hidden_1 = np.dot(dz_hidden_1.T, X)\n",
    "db_hidden_1 = np.sum(dz_hidden_1, axis=0)\n",
    "print(dw_hidden_1)\n",
    "print(db_hidden_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 18px;\">3.3 อัปเดต Weights และ Bias ทั้งหมด </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.1 อัปเดต Weights และ Bias สำหรับ Output Layer\n",
    "\n",
    "เราจะอัปเดตค่า weights (`weights_output`) และ bias (`bias_output`) ของ Output Layer โดยใช้สมการ:\n",
    "\n",
    "$$\n",
    "w_{\\text{output,new}} = w_{\\text{output,old}} - \\alpha \\cdot \\frac{\\partial L}{\\partial w_{\\text{output}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_{\\text{output,new}} = b_{\\text{output,old}} - \\alpha \\cdot \\frac{\\partial L}{\\partial b_{\\text{output}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# อัตราการเรียนรู้ (Learning Rate)\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# อัปเดต Weights และ Bias สำหรับ Output Layer\n",
    "weights_output_reshape -= alpha * dw_output\n",
    "bias_output -= alpha * db_output\n",
    "print(weights_output)\n",
    "print(bias_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.2 อัปเดต Weights และ Bias สำหรับ Hidden Layer\n",
    "\n",
    "เราจะอัปเดตค่า weights (`weights_hidden_1`) และ bias (`biases_hidden_1`) ของ Hidden Layer โดยใช้สมการ:\n",
    "\n",
    "$$\n",
    "w_{\\text{hidden,new}} = w_{\\text{hidden,old}} - \\alpha \\cdot \\frac{\\partial L}{\\partial w_{\\text{hidden}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_{\\text{hidden,new}} = b_{\\text{hidden,old}} - \\alpha \\cdot \\frac{\\partial L}{\\partial b_{\\text{hidden}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# อัปเดต Weights และ Bias สำหรับ Hidden Layer\n",
    "weights_hidden_1 -= alpha * dw_hidden_1\n",
    "biases_hidden_1 -= alpha * db_hidden_1\n",
    "print(weights_hidden_1)\n",
    "print(biases_hidden_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 18px;\">3.4 สร้างฟังก์ชันการเทรนโมเดลแบบสำเร็จรูป </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(z):\n",
    "    return z > 0\n",
    "\n",
    "def train_model(X, y, weights_hidden_1, biases_hidden_1, weights_output, bias_output, lr=0.01, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        # Forward Propagation\n",
    "        z_hidden_1 = np.dot(X, weights_hidden_1.T) + biases_hidden_1 \n",
    "        a_hidden_1 = relu(z_hidden_1)\n",
    "        z_output = np.dot(a_hidden_1, weights_output) + bias_output\n",
    "        a_output = sigmoid(z_output)\n",
    "\n",
    "        # Backward Propagation\n",
    "        dz_output = a_output - y\n",
    "        dw_output = np.dot(a_hidden_1.T, dz_output)\n",
    "        db_output = np.sum(dz_output, axis=0)\n",
    "\n",
    "        dz_output = dz_output.reshape(-1, 1)\n",
    "        weights_output_reshape = weights_output.reshape(1, -1)\n",
    "        dz_hidden_1 = np.dot(dz_output, weights_output_reshape) * relu_derivative(z_hidden_1)\n",
    "        dw_hidden_1 = np.dot(dz_hidden_1.T, X)\n",
    "        db_hidden_1 = np.sum(dz_hidden_1, axis=0) \n",
    "\n",
    "        # อัปเดต Weights และ Biases\n",
    "        weights_output_reshape -= lr * dw_output\n",
    "        bias_output -= lr * db_output\n",
    "\n",
    "        weights_hidden_1 -= lr * dw_hidden_1\n",
    "        biases_hidden_1 -= lr * db_hidden_1  \n",
    "\n",
    "        # แสดงผล loss ระหว่างการเทรน\n",
    "        loss = -np.mean(y * np.log(a_output) + (1 - y) * np.log(1 - a_output))\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss}')\n",
    "\n",
    "    return weights_hidden_1, biases_hidden_1, weights_output, bias_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_hidden_1 = np.array([\n",
    "    [0.5, -0.6, 0.2],  # โหนด 1\n",
    "    [-0.3, 0.8, 0.1]   # โหนด 2\n",
    "])\n",
    "\n",
    "biases_hidden_1 = np.array([-0.1, 0.2])\n",
    "\n",
    "weights_output = np.array([0.4, -0.7])\n",
    "\n",
    "bias_output = -0.3\n",
    "\n",
    "# เทรนโมเดล neural Network\n",
    "weights_hidden_1, biases_hidden_1, weights_output, bias_output = train_model(\n",
    "    X, y, weights_hidden_1, biases_hidden_1, weights_output, bias_output, lr=0.01, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# แสดงผลลัพธ์สุดท้าย\n",
    "print(\"Weights Hidden Layer 1:\")\n",
    "print(weights_hidden_1)\n",
    "print(\"\\nBiases Hidden Layer 1:\")\n",
    "print(biases_hidden_1)\n",
    "print(\"\\nWeights Output Layer:\")\n",
    "print(weights_output)\n",
    "print(\"\\nBias Output Layer:\")\n",
    "print(bias_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
